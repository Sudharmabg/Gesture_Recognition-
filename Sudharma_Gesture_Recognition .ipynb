{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project - Gesture Recognition\n",
    "\n",
    "\n",
    "## Done by Sudharma BG \n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "\n",
    "- Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "\n",
    "- The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "##### Gesture Corresponding Action\n",
    "\n",
    "1. Thumbs Up\tIncrease the volume.\n",
    "2. Thumbs Down\tDecrease the volume.\n",
    "3. Left Swipe\t'Jump' backwards 10 seconds.\n",
    "4. Right Swipe\t'Jump' forward 10 seconds.\n",
    "5. Stop\tPause the movie.\n",
    "\n",
    "Each video is a sequence of 30 frames (or images).\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "- Generator: The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
    "\n",
    "- Model: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries \n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the random seed so that the results don't vary drastically.\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "import keras as Keras\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this block, you read the folder names for training and validation. You also set the batch_size here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with img_idx, y,z and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28]#create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(source_path)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Cropped image of above dimension \n",
    "                    # (It will not change orginal image) \n",
    "                    \n",
    "                    #image = image.crop((0, 0, 120, 120))\n",
    "                    image = image.resize(100, 100)\n",
    "                    \n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] /= 255\n",
    "                    batch_data[folder,idx,:,:,1] /= 255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] /= 255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if((len(source_path)%batch_size)//2==0):\n",
    "            batch_size = 2\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        num_batches = len(source_path)%batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Cropped image of above dimension \n",
    "                    # (It will not change orginal image) \n",
    "                    \n",
    "                    #image = image.crop((0, 0, 120, 120))\n",
    "                    image = image.resize(100, 100)\n",
    "                    \n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] /= 255\n",
    "                    batch_data[folder,idx,:,:,1] /= 255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] /= 255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CNN + RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "Input_shape = (18, 100, 100, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3,3,3), padding='same',\n",
    "                 input_shape=Input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (3, 3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(64, (3, 3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 18, 100, 100, 32)  2624      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 18, 100, 100, 32)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 18, 100, 100, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 16, 98, 98, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 98, 98, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 98, 98, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 8, 49, 49, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 49, 49, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 8, 49, 49, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 49, 49, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 49, 49, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 6, 47, 47, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 47, 47, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 47, 47, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 101568)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               52003328  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 52,202,981\n",
      "Trainable params: 52,202,597\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n",
    "  verbose = 0, mode = \"auto\", min_delta = 1e-04, cooldown = 0,\n",
    "  min_lr = 0)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) - 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:33.581856 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:33.616083 140698242230016 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.60355-1.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6036 - categorical_accuracy: 1.0000\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.60674-0.50000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6067 - categorical_accuracy: 0.5000\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.60523-0.66667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6052 - categorical_accuracy: 0.6667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:34.231240 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:34.282564 140697860552448 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 199ms/step - loss: 1.6052 - categorical_accuracy: 0.6667 - val_loss: 1.6119 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.60486-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6049 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.60350-0.33333.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6035 - categorical_accuracy: 0.3333    \n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.60600-0.20000.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6060 - categorical_accuracy: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:35.434858 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:35.491008 140697751512832 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 406ms/step - loss: 1.6060 - categorical_accuracy: 0.2000 - val_loss: 1.6115 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.61238-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6124 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.61104-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6110 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.60786-0.16667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6079 - categorical_accuracy: 0.1667    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:37.127959 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:37.208512 140697717942016 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 527ms/step - loss: 1.6079 - categorical_accuracy: 0.1667 - val_loss: 1.6083 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.60114-0.50000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6011 - categorical_accuracy: 0.5000\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.60094-0.50000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6009 - categorical_accuracy: 0.5000\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.60673-0.33333.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6067 - categorical_accuracy: 0.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:38.690682 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:38.805011 140697608902400 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 458ms/step - loss: 1.6067 - categorical_accuracy: 0.3333 - val_loss: 1.6063 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.59728-1.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5973 - categorical_accuracy: 1.0000\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.60803-0.50000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6080 - categorical_accuracy: 0.5000\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.60977-0.33333.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6098 - categorical_accuracy: 0.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:40.340898 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:40.433078 140697575331584 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 419ms/step - loss: 1.6098 - categorical_accuracy: 0.3333 - val_loss: 1.6090 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.60935-0.50000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6093 - categorical_accuracy: 0.5000\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.61137-0.25000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6114 - categorical_accuracy: 0.2500\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.61528-0.16667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6153 - categorical_accuracy: 0.1667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:41.706622 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:41.844389 140697119074048 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 450ms/step - loss: 1.6153 - categorical_accuracy: 0.1667 - val_loss: 1.6127 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.61185-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6118 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.61742-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6174 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.61461-0.00000.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6146 - categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:43.379390 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:43.555007 140696879097600 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 536ms/step - loss: 1.6146 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6089 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.60340-0.50000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6034 - categorical_accuracy: 0.5000\n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.60627-0.25000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6063 - categorical_accuracy: 0.2500\n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.61046-0.16667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6105 - categorical_accuracy: 0.1667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:45.365479 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:45.477126 140696845526784 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 617ms/step - loss: 1.6105 - categorical_accuracy: 0.1667 - val_loss: 1.6050 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.60754-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6075 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.61309-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6131 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.61040-0.16667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6104 - categorical_accuracy: 0.1667    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:47.409531 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:47.473240 140696736487168 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 486ms/step - loss: 1.6104 - categorical_accuracy: 0.1667 - val_loss: 1.6091 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.60348-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6035 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.60418-0.25000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6042 - categorical_accuracy: 0.2500    \n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.60570-0.16667.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6057 - categorical_accuracy: 0.1667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:54:49.011877 140702984374080 data_adapter.py:839] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `tf.data.Dataset`.\n",
      "W0614 01:54:49.128597 140696702916352 data_utils.py:522] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3/3 [==============================] - 1s 492ms/step - loss: 1.6057 - categorical_accuracy: 0.1667 - val_loss: 1.6132 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff767e2b160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=3, epochs=num_epochs , \n",
    "                     verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, \n",
    "                    class_weight=None, workers=1, \n",
    "                    initial_epoch=0,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 18, 50, 50, 32)    4736      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 18, 48, 48, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 18, 24, 24, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 18, 24, 24, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 18, 24, 24, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 18, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 18, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 18, 12, 12, 128)   147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 18, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 18, 6, 6, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 18, 6, 6, 256)     590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 18, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 18, 3, 3, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 18, 3, 3, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 18, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 6,817,829\n",
      "Trainable params: 6,817,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "Input_shape_1 = (18, 100, 100, 3)\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), activation='relu', padding='same'), input_shape=Input_shape_1))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=\"he_normal\", activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Conv2D(512, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(512, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    " \n",
    "model.add(TimeDistributed(Flatten()))\n",
    " \n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(512, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 18, 50, 50, 32)    4736      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 18, 48, 48, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 18, 24, 24, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 18, 24, 24, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 18, 24, 24, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 18, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 18, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 18, 12, 12, 128)   147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 18, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 18, 6, 6, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 18, 6, 6, 256)     590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 18, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 18, 3, 3, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 18, 3, 3, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 18, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 6,817,829\n",
      "Trainable params: 6,817,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#optimiser = Keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Gesture_recog' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 10,\n",
    "  verbose = 0, mode = \"auto\", min_delta = 1e-04, cooldown = 0,\n",
    "  min_lr = 0)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 51\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.60944-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.61214-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6121 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00001: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00001-1.61525-0.00000.h5\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6153 - categorical_accuracy: 0.0000e+00Source path =  val ; batch size = 51\n",
      "3/3 [==============================] - 2s 525ms/step - loss: 1.6153 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6282 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.62902-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6290 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.61697-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6170 - categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 01:55:29.413332 140702984374080 callbacks.py:307] Method (on_train_batch_end) is slow compared to the batch update (0.103332). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00002-1.61309-0.00000.h5\n",
      "3/3 [==============================] - 2s 514ms/step - loss: 1.6131 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6258 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.59274-0.50000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5927 - categorical_accuracy: 0.5000\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.59692-0.50000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.5969 - categorical_accuracy: 0.5000\n",
      "Epoch 00003: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00003-1.59214-0.50000.h5\n",
      "3/3 [==============================] - 2s 517ms/step - loss: 1.5921 - categorical_accuracy: 0.5000 - val_loss: 1.6116 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.64089-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6409 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.62116-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6212 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00004: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00004-1.62417-0.00000.h5\n",
      "3/3 [==============================] - 1s 296ms/step - loss: 1.6242 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6189 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.64886-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6489 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.63526-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6353 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00005: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00005-1.63360-0.00000.h5\n",
      "3/3 [==============================] - 1s 441ms/step - loss: 1.6336 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6028 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.60619-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6062 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.61479-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6148 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00006: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00006-1.60580-0.33333.h5\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 1.6058 - categorical_accuracy: 0.3333 - val_loss: 1.6183 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.60357-0.50000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6036 - categorical_accuracy: 0.5000\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.61826-0.25000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6183 - categorical_accuracy: 0.2500\n",
      "Epoch 00007: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00007-1.60702-0.33333.h5\n",
      "3/3 [==============================] - 1s 437ms/step - loss: 1.6070 - categorical_accuracy: 0.3333 - val_loss: 1.6023 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.65106-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6511 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.60615-0.50000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6061 - categorical_accuracy: 0.5000    \n",
      "Epoch 00008: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00008-1.61535-0.33333.h5\n",
      "3/3 [==============================] - 2s 597ms/step - loss: 1.6153 - categorical_accuracy: 0.3333 - val_loss: 1.6107 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.62012-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6201 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.62316-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6232 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00009: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00009-1.61833-0.00000.h5\n",
      "3/3 [==============================] - 1s 452ms/step - loss: 1.6183 - categorical_accuracy: 0.0000e+00 - val_loss: 1.6008 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.59929-0.00000.h5\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5993 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.62739-0.00000.h5\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 1.6274 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 00010: saving model to Gesture_recog_2021-06-1401_53_04.833086/model-00010-1.61691-0.16667.h5\n",
      "3/3 [==============================] - 1s 278ms/step - loss: 1.6169 - categorical_accuracy: 0.1667 - val_loss: 1.5867 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff74c2df198>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=3, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None,initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
